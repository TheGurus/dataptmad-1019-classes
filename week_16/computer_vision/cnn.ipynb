{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convolutional neural networks, guided lesson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, most commonly applied to analyzing visual imagery:\n",
    "\n",
    "* they are also known as shift invariant or space invariant artificial neural networks (SIANN)\n",
    "* based on their shared-weights architecture and translation invariance characteristics. \n",
    "* They have applications in:\n",
    "    * image and video recognition \n",
    "    * recommender systems\n",
    "    * image classification\n",
    "    * medical image analysis\n",
    "    * natural language processing\n",
    "    * and financial time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to install: `opencv-python`, `tensorflow`, `keras`, and its requirements... take into account that `opencv` is hard to make it work properly, even for experienced developers..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. convolution operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is a convolution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![convolution](https://miro.medium.com/max/1920/1*D6iRfzDkz-sEzyjYoVZ73w.gif \"convolution operation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cnn features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![convolution](https://www.pnas.org/content/pnas/116/4/1074/F2.large.jpg?width=800&height=600&carousel=1 \"cnn features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. mnist example with convolutional neural network and keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "NUM_CLASSES = 10\n",
    "EPOCHS = 12\n",
    "\n",
    "IMG_ROWS, IMG_COLS = 28, 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras includes some dataset to play with..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's visualize some training examples..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "values for an image range from intensity 0 (min) to 255 (max)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  55,\n",
       "        148, 210, 253, 253, 113,  87, 148,  55,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  87, 232,\n",
       "        252, 253, 189, 210, 252, 252, 253, 168,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   4,  57, 242, 252,\n",
       "        190,  65,   5,  12, 182, 252, 253, 116,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  96, 252, 252, 183,\n",
       "         14,   0,   0,  92, 252, 252, 225,  21,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0, 132, 253, 252, 146,  14,\n",
       "          0,   0,   0, 215, 252, 252,  79,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0, 126, 253, 247, 176,   9,   0,\n",
       "          0,   8,  78, 245, 253, 129,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  16, 232, 252, 176,   0,   0,   0,\n",
       "         36, 201, 252, 252, 169,  11,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  22, 252, 252,  30,  22, 119, 197,\n",
       "        241, 253, 252, 251,  77,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  16, 231, 252, 253, 252, 252, 252,\n",
       "        226, 227, 252, 231,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  55, 235, 253, 217, 138,  42,\n",
       "         24, 192, 252, 143,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         62, 255, 253, 109,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         71, 253, 252,  21,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0, 253, 252,  21,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         71, 253, 252,  21,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        106, 253, 252,  21,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         45, 255, 253,  21,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0, 218, 252,  56,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  96, 252, 189,  42,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  14, 184, 252, 170,  11,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  14, 147, 252,  42,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[4]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now, let's reshape data to take into account channels, even if images are gray scale... this depend on chosen backend for Keras, commonly Tensorflow, which is `channel last` ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, IMG_ROWS, IMG_COLS)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, IMG_ROWS, IMG_COLS)\n",
    "    INPUT_SHAPE = (1, IMG_ROWS, IMG_COLS)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], IMG_ROWS, IMG_COLS, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], IMG_ROWS, IMG_COLS, 1)\n",
    "    INPUT_SHAPE = (IMG_ROWS, IMG_COLS, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's scale image to have values between 0 and 1..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finally, we have a dataset ready to train and test..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = to_categorical(y_train, NUM_CLASSES)\n",
    "y_test = to_categorical(y_test, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adadelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=INPUT_SHAPE))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(NUM_CLASSES, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=categorical_crossentropy,\n",
    "              optimizer=Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's train the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 16s 274us/step - loss: 0.2639 - accuracy: 0.9177 - val_loss: 0.0569 - val_accuracy: 0.9821\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 14s 230us/step - loss: 0.0897 - accuracy: 0.9732 - val_loss: 0.0374 - val_accuracy: 0.9875\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 14s 233us/step - loss: 0.0643 - accuracy: 0.9804 - val_loss: 0.0354 - val_accuracy: 0.9885\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 14s 235us/step - loss: 0.0538 - accuracy: 0.9840 - val_loss: 0.0310 - val_accuracy: 0.9894\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 14s 235us/step - loss: 0.0475 - accuracy: 0.9861 - val_loss: 0.0287 - val_accuracy: 0.9901\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 14s 233us/step - loss: 0.0420 - accuracy: 0.9875 - val_loss: 0.0312 - val_accuracy: 0.9897\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 14s 234us/step - loss: 0.0380 - accuracy: 0.9885 - val_loss: 0.0257 - val_accuracy: 0.9910\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 14s 235us/step - loss: 0.0323 - accuracy: 0.9904 - val_loss: 0.0269 - val_accuracy: 0.9914\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 14s 236us/step - loss: 0.0295 - accuracy: 0.9913 - val_loss: 0.0268 - val_accuracy: 0.9914\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 14s 235us/step - loss: 0.0276 - accuracy: 0.9912 - val_loss: 0.0286 - val_accuracy: 0.9910\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 14s 239us/step - loss: 0.0262 - accuracy: 0.9918 - val_loss: 0.0277 - val_accuracy: 0.9911\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 14s 240us/step - loss: 0.0238 - accuracy: 0.9922 - val_loss: 0.0252 - val_accuracy: 0.9920\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fafdab0ddd0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.025446641624959195\n",
      "Test accuracy: 0.9922999739646912\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. fully connected neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Flatten(input_shape=INPUT_SHAPE))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(NUM_CLASSES, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_10 (Flatten)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 116,362\n",
      "Trainable params: 116,362\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=categorical_crossentropy,\n",
    "              optimizer=Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 1.8727 - accuracy: 0.3406 - val_loss: 1.1472 - val_accuracy: 0.6291\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 1.2724 - accuracy: 0.5376 - val_loss: 0.9002 - val_accuracy: 0.6869\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 1.0827 - accuracy: 0.6094 - val_loss: 0.8196 - val_accuracy: 0.7311\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.9990 - accuracy: 0.6447 - val_loss: 0.7896 - val_accuracy: 0.7590\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.9475 - accuracy: 0.6768 - val_loss: 0.7045 - val_accuracy: 0.7990\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.8971 - accuracy: 0.7047 - val_loss: 0.6777 - val_accuracy: 0.8181\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.8541 - accuracy: 0.7276 - val_loss: 0.6437 - val_accuracy: 0.8428\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.8170 - accuracy: 0.7460 - val_loss: 0.6103 - val_accuracy: 0.8736\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.7883 - accuracy: 0.7621 - val_loss: 0.5844 - val_accuracy: 0.8732\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.7726 - accuracy: 0.7721 - val_loss: 0.5645 - val_accuracy: 0.8962\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.7529 - accuracy: 0.7844 - val_loss: 0.5350 - val_accuracy: 0.8918\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.7254 - accuracy: 0.7908 - val_loss: 0.5304 - val_accuracy: 0.8965\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f41687131d0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ironhack_env]",
   "language": "python",
   "name": "conda-env-ironhack_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
