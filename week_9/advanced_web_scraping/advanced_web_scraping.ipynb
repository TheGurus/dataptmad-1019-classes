{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Web Response Status Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/List_of_HTTP_status_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1 = requests.get('https://thegurus.tech')\n",
    "r1.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "404"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2 = requests.get('https://thegurus.tech/i_dont_exist')\n",
    "r2.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r3 = requests.get('http://thegurus.tech')\n",
    "r3.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Response [301]>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r3.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r3.history[0].status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<html>\\r\\n<head><title>301 Moved Permanently</title></head>\\r\\n<body bgcolor=\"white\">\\r\\n<center><h1>301 Moved Permanently</h1></center>\\r\\n<hr><center>nginx/1.14.0 (Ubuntu)</center>\\r\\n</body>\\r\\n</html>\\r\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r3.history[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r4 = requests.get('https://thegurus.tech')\n",
    "r4.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a good idea to check the status code before you parse the response text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request was successful\n"
     ]
    }
   ],
   "source": [
    "url = 'http://thegurus.tech'\n",
    "\n",
    "r = requests.get(url)\n",
    "\n",
    "if r.status_code < 300:\n",
    "    print('request was successful')\n",
    "elif r.status_code >= 300 and r.status_code < 400:\n",
    "    print('request was redirected')\n",
    "elif r.status_code >= 400 and r.status_code < 500:\n",
    "    print('request failed because the resource either does not exist or is forbidden')\n",
    "elif r.status_code >= 500:\n",
    "    print('request failed because the response server encountered an error')\n",
    "else:\n",
    "    print('we have found a new http protocol')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be noted that the requests lib automatically makes additional requests to the redirected URL if the web resource is moved (i.e. 30x status codes). Even if a moved web resource redirects once again, the requests lib will track it all the way down until it receives success or failure as long as the number of redirects does not exceed the redirect limit (default 30). You can choose to disallow redirects by using requests.get(url, allow_redirects=False) so that requests will not track down the redirected URL. Or you can choose to reduce the max redirects allowed by using max_redirects=n so as to avoid endless redirects or save time in making requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = requests.get('http://thegurus.tech', allow_redirects=False)\n",
    "r.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Request Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exceeding Max Redirects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously mentioned, there is a max redirects number by default (30) which you can override with max_redirects. If the number of redirects exceeds the limit, requests will throw a TooManyRedirects exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful request!\n"
     ]
    }
   ],
   "source": [
    "session = requests.Session()\n",
    "session.max_redirects = 1\n",
    "\n",
    "try:\n",
    "    session.get('http://thegurus.tech').status_code\n",
    "    print('successful request!')\n",
    "    \n",
    "except requests.exceptions.TooManyRedirects as ex:\n",
    "    print('handled exception!')\n",
    "    print(ex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timeout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes a remote server is not responsive either because requests cannot connect to the intended web resource or because the remote server does not send back the promised data. If that happens, requests will typically wait for a long period of time until the connection is closed by the remote server then throw a ConnectTimeout exception. This is a big waste of time because most modern websites respond to web requests within a couple of seconds. Therefore, it's a desirable approach to supply a timeout argument to requests to limit the amount of wait time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handled timeout exception!\n",
      "HTTPSConnectionPool(host='thegurus.tech', port=443): Read timed out. (read timeout=0.05)\n",
      "finally executed\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    requests.get('https://thegurus.tech', timeout=0.05)\n",
    "    print('successful request!')\n",
    "    \n",
    "except requests.exceptions.Timeout as ex:\n",
    "    print('handled timeout exception!')\n",
    "    print(ex)\n",
    "\n",
    "except requests.exceptions.ConnectionError as ex:\n",
    "    print('handled connection exception!')\n",
    "    print(ex)\n",
    "\n",
    "finally:\n",
    "    print('finally executed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSL Certificate Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a website wants to use SSL/TLS, it has to purchase (or not -> Let's Encrypt) a special certificate from certificate vendors and configure the web server properly in order for the certificate to be functional. If the SSL/TLS certificate is not installed properly or it has expired (purchased certificate has to be renewed every two years), modern browsers such as Chrome will indicate the problem to the users. The requests lib, similarly, will throw an exception if it detects the SSL certificate is problematic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful request!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    requests.get('https://thegurus.tech')\n",
    "    print('successful request!')\n",
    "    \n",
    "except requests.exceptions.SSLError as ex:\n",
    "    print('handled exception!')\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifies the browser and operating system from which the request is made. Some websites send out different responses to the requests made with different user agents for reasons such as:\n",
    "\n",
    "- They want to avoid bugs of the website that only occur in certain browsers or browser versions.\n",
    "\n",
    "- They want to personalize user experience (e.g. desktop vs mobile, small vs big screen, etc.) by sending different data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you decide to fool the website by pretending to be a certain browser, use the approach below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<!DOCTYPE html>\\n<html lang=\"en\">\\n\\n<head>\\n        <meta charset=\"utf-8\">\\n        <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\\n        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\\n\\n\\n        <title>The Gurus</title>\\n\\n            <link href=\"https://thegurus.tech/feeds/all.atom.xml\" type=\"application/atom+xml\" rel=\"alternate\" title=\"The Gurus Full Atom Feed\" />\\n        <!-- Bootstrap Core CSS -->\\n        <link href=\"https://thegurus.tech/theme/css/bootstrap.min.css\" '"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'}\n",
    "response = requests.get('https://thegurus.tech', headers=headers)\n",
    "response.content[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the user agent string above, the request pretends to be from Chrome browser v71.0.3578.98 in macOS 10.14.2 (Mojave). To see what the user agent string look like in other browsers/OS, check out https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/User-Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Asynchronous Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: asyncio in /home/ubuntu/miniconda3/envs/ironhack_env/lib/python3.7/site-packages (3.4.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __async is not about paralelizing tasks, it's about scheduling tasks__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import asyncio\n",
    "import requests\n",
    "\n",
    "N_REQUESTS = 5\n",
    "\n",
    "async def gurus_async():\n",
    "    print(requests.get('https://thegurus.tech'))\n",
    "    await asyncio.sleep(0)\n",
    "    print(requests.get('http://thegurus.tech', allow_redirects=False))\n",
    "    await asyncio.sleep(0)\n",
    "    print(requests.get('https://thegurus.tech/i_dont_exist'))\n",
    "\n",
    "async def main_async():\n",
    "\n",
    "    function_list = [gurus_async() for _ in range(N_REQUESTS)]\n",
    "    await asyncio.gather(*function_list)\n",
    "\n",
    "def gurus_sync():\n",
    "    print(requests.get('https://thegurus.tech'))\n",
    "    print(requests.get('http://thegurus.tech', allow_redirects=False))\n",
    "    print(requests.get('https://thegurus.tech/i_dont_exist'))\n",
    "\n",
    "def main_sync():\n",
    "\n",
    "    for _ in range(N_REQUESTS):\n",
    "        gurus_sync()\n",
    "\n",
    "print('\\nrunning async...\\n')\n",
    "asyncio.run(main_async())\n",
    "\n",
    "print('\\nrunning sync...\\n')\n",
    "main_sync()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cannot be run on Jupyter Notebook because asyncio.run cannot be called when another asyncio event loop is running in the same thread (Jupyter Notebook asyncio event)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding robots.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many reasons why some websites don't welcome bots:\n",
    "\n",
    "- When bots crawls a website, it takes up some CPU, memory, and bandwidth resources of the server that should have been dedicated to the normal users. \n",
    "- Sometimes the website admin does not want to expose certain semi-confidential web resources to the search engines. \n",
    "- Sometimes the website admin wants to promote the most important web resources rather than letting the search engines see many irrelevant or outdated resources resting on the server. \n",
    "\n",
    "To achieve those different purposes, the common practice is to create a robots.txt file in the root level of the website to instruct the bots what to crawl and what not to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In case of \"not malicious\" crawlers: Google?\n",
    "\n",
    "The robots.txt file is there to tell crawlers and robots which URLs they should not visit on your website. This is important to help them avoid crawling low quality pages, or getting stuck in crawl traps where an infinite number of URLs could potentially be created, for example, a calendar section which creates a new URL for every day.\n",
    "\n",
    "- In case of \"malicious\" crawlers:\n",
    "\n",
    "They will give a shit about your robots.txt XD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more info about robots.txt please have a look at: https://support.google.com/webmasters/answer/6062608?hl=en&ref_topic=6061961&visit_id=637139316066285039-3854058931&rd=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ironhack_env]",
   "language": "python",
   "name": "conda-env-ironhack_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
